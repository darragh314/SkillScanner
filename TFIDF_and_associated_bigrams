import requests
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk import bigrams
from collections import defaultdict

def get_wiki_text(topic):
    url = f'https://en.wikipedia.org/wiki/{topic}'
    page = requests.get(url)
    soup = BeautifulSoup(page.content, 'html.parser')
    text = soup.get_text()
    return text

def get_keywords(topics):
    texts = []
    for topic in topics:
        text = get_wiki_text(topic)
        text = [word for word in text.split() if word not in stopwords.words('english')]
        bigrams_list = list(bigrams(text))
        bigrams_list = [bigram for bigram in bigrams_list if bigram[0] != bigram[1]]
        bigrams_string = " ".join([" ".join(bigram) for bigram in bigrams_list])
        texts.append(bigrams_string)
    tfidf = TfidfVectorizer(ngram_range=(2, 2))
    tfidf_matrix = tfidf.fit_transform(texts)
    keywords = []
    for i, topic in enumerate(topics):
        feature_index = tfidf_matrix[i,:].nonzero()[1]
        tfidf_scores = [(index, tfidf_matrix[i, index]) for index in feature_index]
        sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)
        feature_names = {v: k for k, v in tfidf.vocabulary_.items()}
        topic_keywords = [feature_names[index] for index, _ in sorted_scores[:3]]
        keywords.append(topic_keywords)
    return keywords

keywords = get_keywords(skill_labels)
for topic, keyword in zip(skill_labels, keywords):
    print(f'Keywords for {topic}: {keyword}')
